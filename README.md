# emotion-driven-face
이 프로젝트는 감정 인식 결과를 물리적 표현으로 연결하기 위한 기본 구조를 구현한 시스템입니다.
라즈베리파이와 아두이노를 중심으로, 감정 인식 → 상태 판단 → 하드웨어 출력의 흐름을 명확히 분리하여 구성되었습니다.

현재 구현은 단순하지만,
각 구성 요소가 독립적으로 동작하도록 설계되어 기능 확장이나 교체가 비교적 수월한 구조를 갖고 있습니다.


## Overview

시스템은 카메라 입력을 통해 얼굴을 검출하고,
사전 학습된 감정 분류 모델을 이용해 현재 감정 상태를 추정합니다.

추정된 감정은 중앙 제어 프로그램을 통해 다음 두 출력으로 전달됩니다.

- 하나는 아두이노로 전송되어 서보모터의 각도를 조절하고,
- 다른 하나는 LED 제어 프로그램으로 전달되어 입 모양 애니메이션을 출력합니다.

이 과정에서 각 모듈은 역할 중심으로 분리되어 있으며,
입력·판단·출력 단계가 명확하게 나뉘어 있습니다.


## Design Focus

이 프로젝트의 핵심은 개별 기능의 정교함보다는 전체 시스템을 구성하는 구조와 흐름에 있습니다.

감정 인식 로직은 독립된 모듈로 분리되어 있어 다른 모델이나 입력 방식으로 교체가 가능합니다. 하드웨어 제어는 감정 결과를 문자열 또는 숫자 값으로만 전달받아 내부 구현과 느슨하게 결합되어 있습니다. 중앙 제어부는 감정 변화 시점만을 감지해 반응하도록 설계되어 불필요한 하드웨어 동작을 줄입니다. 

이러한 구조는 기능 추가 시 기존 코드를 크게 수정하지 않고도 새로운 출력 장치나 로직을 덧붙일 수 있도록 돕습니다.


## Possible Extensions

현재 구조를 기반으로 다음과 같은 확장이 가능합니다.

- 감정 인식 모델 교체 또는 멀티 감정 확률 처리
- 음성 입력 또는 음성 합성과의 연동
- 표정 출력 장치(눈, 목, 팔 등) 추가
- 원격 제어 또는 네트워크 기반 감정 입력
- 상태 머신 기반의 복합 감정 표현

본 프로젝트는 이러한 확장을 위한 출발점 역할을 의도하고 있습니다.


## System Components

- Raspberry Pi
  - 카메라 입력 처리
  - 감정 인식
  - 전체 시스템을 조율하는 중앙 제어 역할
- Arduino
  - 서보모터 제어 전담
  - 단순한 각도 명령을 받아 물리적 움직임으로 변환
- LED Controller (C / SPI)
  - MAX7219 기반 LED 매트릭스 제어
  - 감정 상태에 따른 프레임 애니메이션 출력

각 구성 요소는 서로의 내부 구현을 알 필요 없이 동작합니다.


## Hardware Interface Overview

![회로도](https://github.com/user-attachments/assets/7dd5fd48-d8e5-46c2-9353-fc99578958bc)

본 시스템의 하드웨어 연결은 각 보드에 충분한 전력 확보, 라즈베리파이와 아두이노 간의 역할 분리를 전제로 구성되었습니다.
라즈베리파이는 감정 인식 및 전체 제어를 담당하고, 아두이노는 서보모터와 같은 실시간 제어가 필요한 출력 장치를 전담합니다.

두 보드는 USB 시리얼 통신을 통해 연결되며,
라즈베리파이는 감정 상태에 따라 계산된 목표 각도를 단순한 숫자 명령으로 전송합니다.
아두이노는 해당 값을 수신한 후 내부 로직에 따라 서보모터를 부드럽게 이동시킵니다.

아래 회로도는 현재 구현에서 사용된 기본적인 연결 구조를 나타냅니다.
이 회로는 최소 구성만을 포함하고 있으며, 출력 장치 추가나 전원 분리와 같은 확장은 쉽게 이루어질 수 있습니다.


## Scope & Limitations

현재 구현은 단일 얼굴과 단순한 감정 분류를 기준으로 동작하며,
정확도나 표현의 다양성보다는 구조 검증에 초점을 맞추고 있습니다.


## License

This project is licensed under the MIT License.
